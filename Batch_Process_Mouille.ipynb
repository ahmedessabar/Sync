{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Traitement par Lot + Rapport CSV (Mouille)\n",
                "\n",
                "Ce notebook traite automatiquement tous les fichiers du dossier `Moto_06112025_Chicane_Mouille`.\n",
                "\n",
                "**Mise à jour V2** :\n",
                "Inclut la logique de **Fallback Force Sync** et de **Validation de Longueur** (comme pour les dossiers Sec et Freinage), pour garantir une robustesse maximale même en cas de dérive d'horloge future."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from nptdms import TdmsFile\n",
                "from scipy.interpolate import interp1d\n",
                "from datetime import datetime, timedelta\n",
                "import os\n",
                "import glob\n",
                "import re\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "BASE_DIR = r'Moto_06112025_Chicane_Mouille'\n",
                "DIR_TXT = os.path.join(BASE_DIR, 'Moto_chicane_mouillee_TXT')\n",
                "DIR_TDMS = os.path.join(BASE_DIR, 'labview_data') \n",
                "DIR_OUT = os.path.join(BASE_DIR, 'Merged_CSV')\n",
                "\n",
                "TDMS_FREQ = 400.0\n",
                "MAGIC_OFFSET = 0.2679\n",
                "\n",
                "os.makedirs(DIR_OUT, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_xsens(path):\n",
                "    h_idx = None\n",
                "    try:\n",
                "        with open(path, 'r', errors='ignore') as f:\n",
                "            for i, l in enumerate(f):\n",
                "                if l.strip().startswith('PacketCounter'): h_idx=i; break\n",
                "        if h_idx is None:\n",
                "             with open(path, 'r', errors='ignore') as f:\n",
                "                for i, l in enumerate(f): \n",
                "                    if 'UTC_Year' in l: h_idx=i; break\n",
                "        if h_idx is None: return pd.DataFrame()\n",
                "        \n",
                "        try: df = pd.read_csv(path, sep='\\t', header=h_idx)\n",
                "        except: df = pd.read_csv(path, sep=r'\\s+', header=h_idx)\n",
                "        \n",
                "        df.columns = df.columns.str.strip()\n",
                "        # Cleaning Ghost Packets\n",
                "        check_cols = [c for c in ['Acc_X', 'FreeAcc_E', 'Gyr_X'] if c in df.columns]\n",
                "        if check_cols:\n",
                "            df.dropna(subset=check_cols, how='all', inplace=True)\n",
                "            \n",
                "        req = ['UTC_Year', 'UTC_Month', 'UTC_Day', 'UTC_Hour', 'UTC_Minute', 'UTC_Second', 'UTC_Nano']\n",
                "        if set(req).issubset(df.columns):\n",
                "            df.dropna(subset=req, inplace=True)\n",
                "            ts = pd.to_datetime(df[req[:-1]].astype(int).rename(columns={'UTC_Year':'year','UTC_Month':'month','UTC_Day':'day','UTC_Hour':'hour','UTC_Minute':'minute','UTC_Second':'second'}))\n",
                "            df['TS_UTC'] = ts + pd.to_timedelta(df['UTC_Nano'], unit='ns')\n",
                "            df.drop(columns=req, inplace=True)\n",
                "            df.sort_values('TS_UTC', inplace=True)\n",
                "            df.drop_duplicates(subset=['TS_UTC'], inplace=True)\n",
                "            return df\n",
                "    except: return pd.DataFrame()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_tdms_smart(path, group_name, xsens_start_ref):\n",
                "    stats = {\n",
                "        \"TDMS_Found\": False, \"Reset_Detected\": False, \"Reset_Index\": 0,\n",
                "        \"Sync_Method\": \"Unknown\", \"TDMS_Start_Time\": None, \"TDMS_Points_Valid\": 0\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        tdms = TdmsFile.read(path)\n",
                "        stats[\"TDMS_Found\"] = True\n",
                "        \n",
                "        target_group = None\n",
                "        for g in tdms.groups():\n",
                "            if g.name == group_name: target_group = g; break\n",
                "        if not target_group: return pd.DataFrame(), stats\n",
                "        \n",
                "        # 1. Données Brutes\n",
                "        data = {}\n",
                "        for c in target_group.channels(): data[c.name] = c[:]\n",
                "        if not data: return pd.DataFrame(), stats\n",
                "        l_min = min(len(v) for v in data.values())\n",
                "        data = {k: v[:l_min] for k,v in data.items()}\n",
                "        df = pd.DataFrame(data)\n",
                "        \n",
                "        base_start_time = None\n",
                "        \n",
                "        # 2. Detection Reset\n",
                "        start_index = 0\n",
                "        if 'Edges_RoueAR' in df.columns:\n",
                "            diffs = np.diff(df['Edges_RoueAR'].values)\n",
                "            resets = np.where(diffs < -100)[0]\n",
                "            if len(resets) > 0:\n",
                "                start_index = resets[0] + 1\n",
                "                stats[\"Reset_Detected\"] = True\n",
                "                stats[\"Reset_Index\"] = int(start_index)\n",
                "\n",
                "        # 3. Stratégie Timestamp\n",
                "        if stats[\"Reset_Detected\"]:\n",
                "            # FORCED SYNC\n",
                "            base_start_time = xsens_start_ref + timedelta(seconds=MAGIC_OFFSET)\n",
                "            stats[\"Sync_Method\"] = \"Forced (Xsens+Offset)\"\n",
                "        else:\n",
                "            # METADATA SYNC\n",
                "            if 'Edges_RoueAR' in target_group:\n",
                "                chan = target_group['Edges_RoueAR']\n",
                "                if 'wf_start_time' in chan.properties:\n",
                "                     base_start_time = pd.to_datetime(chan.properties['wf_start_time']).tz_localize(None)\n",
                "            if not base_start_time and 'wf_start_time' in tdms.properties:\n",
                "                 base_start_time = pd.to_datetime(tdms.properties['wf_start_time']).tz_localize(None)\n",
                "            \n",
                "            stats[\"Sync_Method\"] = \"Metadata\"\n",
                "            \n",
                "        stats[\"TDMS_Start_Time\"] = base_start_time\n",
                "        \n",
                "        # 4. Slice if reset\n",
                "        if start_index > 0:\n",
                "            df = df.iloc[start_index:].copy().reset_index(drop=True)\n",
                "            \n",
                "        stats[\"TDMS_Points_Valid\"] = len(df)\n",
                "            \n",
                "        # 5. Index Create\n",
                "        if base_start_time:\n",
                "            time_index = pd.date_range(start=base_start_time, periods=len(df), freq=f'{1000/TDMS_FREQ}ms')\n",
                "            df['TDMS_Timestamp'] = time_index\n",
                "            df.set_index('TDMS_Timestamp', inplace=True)\n",
                "            df.columns = [f\"TDMS_{c}\" for c in df.columns]\n",
                "            return df, stats\n",
                "        else:\n",
                "            stats[\"Sync_Method\"] = \"FAILED (No Time)\"\n",
                "            return pd.DataFrame(), stats\n",
                "\n",
                "    except Exception as e: \n",
                "        stats[\"Error\"] = str(e)\n",
                "        return pd.DataFrame(), stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- BATCH PROCESSING ---\n",
                "txt_files = glob.glob(os.path.join(DIR_TXT, \"*.txt\"))\n",
                "print(f\"Fichiers trouvés : {len(txt_files)}\")\n",
                "\n",
                "LOG_DATA = []\n",
                "\n",
                "for f_txt in txt_files:\n",
                "    basename = os.path.basename(f_txt)\n",
                "    \n",
                "    # Init Log Entry\n",
                "    entry = {\n",
                "        \"File_Name\": basename,\n",
                "        \"Status\": \"Init\",\n",
                "        \"Reset_Detected\": False, \n",
                "        \"Sync_Strategy\": None,\n",
                "        \"Xsens_Start\": None, \n",
                "        \"TDMS_Start\": None,\n",
                "        \"Xsens_Points\": 0,\n",
                "        \"TDMS_Points\": 0,\n",
                "        \"Reset_Index\": 0\n",
                "    }\n",
                "    \n",
                "    match = re.search(r'mouille_(\\d+)_(\\w+)\\.txt$', basename)\n",
                "    if not match:\n",
                "        print(f\"[SKIP] Format nom incorrect : {basename}\")\n",
                "        entry[\"Status\"] = \"Skipped (Name Format)\"\n",
                "        LOG_DATA.append(entry)\n",
                "        continue\n",
                "        \n",
                "    speed = match.group(1)\n",
                "    group = match.group(2)\n",
                "    \n",
                "    tdms_name = f\"Moto_Chicane_mouille_{speed}.tdms\"\n",
                "    f_tdms = os.path.join(DIR_TDMS, tdms_name)\n",
                "    \n",
                "    if not os.path.exists(f_tdms):\n",
                "        print(f\"[ERR ] TDMS manquant : {tdms_name}\")\n",
                "        entry[\"Status\"] = \"Missing TDMS\"\n",
                "        LOG_DATA.append(entry)\n",
                "        continue\n",
                "        \n",
                "    print(f\"Traitement : {basename} ...\", end='')\n",
                "    \n",
                "    # 1. Load Xsens\n",
                "    df_xsens = load_xsens(f_txt)\n",
                "    if df_xsens.empty:\n",
                "        print(\" [ERR Xsens]\")\n",
                "        entry[\"Status\"] = \"Error Xsens Load\"\n",
                "        LOG_DATA.append(entry)\n",
                "        continue\n",
                "    \n",
                "    entry[\"Xsens_Start\"] = df_xsens['TS_UTC'].min()\n",
                "    entry[\"Xsens_Points\"] = len(df_xsens)\n",
                "        \n",
                "    # 2. Load TDMS (Smart)\n",
                "    xsens_start = df_xsens['TS_UTC'].min()\n",
                "    df_tdms, stats = load_tdms_smart(f_tdms, group, xsens_start)\n",
                "    \n",
                "    entry[\"TDMS_Start\"] = stats.get(\"TDMS_Start_Time\")\n",
                "    entry[\"TDMS_Points\"] = stats.get(\"TDMS_Points_Valid\")\n",
                "    entry[\"Reset_Detected\"] = stats.get(\"Reset_Detected\")\n",
                "    entry[\"Reset_Index\"] = stats.get(\"Reset_Index\")\n",
                "    entry[\"Sync_Strategy\"] = stats.get(\"Sync_Method\")\n",
                "    \n",
                "    if df_tdms.empty:\n",
                "        print(f\" [ERR TDMS]\")\n",
                "        entry[\"Status\"] = \"Error TDMS Load\"\n",
                "        LOG_DATA.append(entry)\n",
                "        continue\n",
                "        \n",
                "    # 3. Merge\n",
                "    t_start = max(df_xsens['TS_UTC'].min(), df_tdms.index.min())\n",
                "    t_end = min(df_xsens['TS_UTC'].max(), df_tdms.index.max())\n",
                "    \n",
                "    # --- CHECK OVERLAP & RETRY ---\n",
                "    overlap_ok = True\n",
                "    if t_end < t_start: overlap_ok = False\n",
                "    \n",
                "    if not overlap_ok:\n",
                "        print(f\" [WARN No Overlap] -> Metadata: {stats['TDMS_Start_Time']} vs Xsens: {xsens_start}\")\n",
                "        \n",
                "        # VALIDATION LONGUEUR\n",
                "        len_xs = len(df_xsens)\n",
                "        len_td = len(df_tdms)\n",
                "        diff_rel = abs(len_xs - len_td) / max(len_xs, 1)\n",
                "        \n",
                "        if diff_rel > 0.15: # 15% Tolérrance\n",
                "            print(f\"   -> [ERR] Length Mismatch ({len_xs} vs {len_td}). Diff={diff_rel:.1%}. ABORT.\")\n",
                "            entry[\"Status\"] = f\"Invalid (Length Mismatch {diff_rel:.0%})\"\n",
                "            LOG_DATA.append(entry)\n",
                "            continue\n",
                "            \n",
                "        print(\"   -> RETRY with Forced Sync (Xsens + Offset)...\", end='')\n",
                "        \n",
                "        # FALLBACK FORCE SYNC\n",
                "        forced_start = entry[\"Xsens_Start\"] + timedelta(seconds=MAGIC_OFFSET)\n",
                "        \n",
                "        # Re-index TDMS\n",
                "        time_index = pd.date_range(start=forced_start, periods=len(df_tdms), freq=f'{1000/TDMS_FREQ}ms')\n",
                "        df_tdms = df_tdms.reset_index(drop=True)\n",
                "        df_tdms['TDMS_Timestamp'] = time_index\n",
                "        df_tdms.set_index('TDMS_Timestamp', inplace=True)\n",
                "        \n",
                "        # Update Stats\n",
                "        entry[\"Sync_Strategy\"] += \" + FALLBACK (Force)\"\n",
                "        stats[\"TDMS_Start_Time\"] = forced_start\n",
                "        entry[\"TDMS_Start\"] = forced_start\n",
                "        \n",
                "        # Re-calc Interval\n",
                "        t_start = max(df_xsens['TS_UTC'].min(), df_tdms.index.min())\n",
                "        t_end = min(df_xsens['TS_UTC'].max(), df_tdms.index.max())\n",
                "        \n",
                "        if t_end < t_start:\n",
                "            print(\" [ERR SYNC FAILED AGAIN]\")\n",
                "            entry[\"Status\"] = \"Sync Error (Even after Force)\"\n",
                "            LOG_DATA.append(entry)\n",
                "            continue\n",
                "\n",
                "    df_merged = df_xsens[(df_xsens['TS_UTC'] >= t_start) & (df_xsens['TS_UTC'] <= t_end)].copy()\n",
                "    df_merged.set_index('TS_UTC', inplace=True)\n",
                "    \n",
                "    try:\n",
                "        t_slave = (df_tdms.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ns') / 1e9\n",
                "        t_master = (df_merged.index - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1ns') / 1e9\n",
                "        \n",
                "        for col in df_tdms.columns:\n",
                "            f = interp1d(t_slave, df_tdms[col].values, kind='linear', fill_value=\"extrapolate\")\n",
                "            df_merged[col] = f(t_master)\n",
                "            \n",
                "        out_name = basename.replace('.txt', '_merged.csv')\n",
                "        out_path = os.path.join(DIR_OUT, out_name)\n",
                "        df_merged.to_csv(out_path, date_format='%d/%m/%Y %H:%M:%S.%f')\n",
                "        \n",
                "        print(f\" [OK] -> {entry['Sync_Strategy']}\")\n",
                "        entry[\"Status\"] = \"Success\"\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\" [ERR MERGE: {e}]\")\n",
                "        entry[\"Status\"] = f\"Merge Exception: {e}\"\n",
                "\n",
                "    LOG_DATA.append(entry)\n",
                "\n",
                "# SAVE LOG REPORT\n",
                "df_log = pd.DataFrame(LOG_DATA)\n",
                "log_path = os.path.join(DIR_OUT, 'Batch_Report.csv')\n",
                "df_log.to_csv(log_path, index=False)\n",
                "print(f\"\\nRapport généré : {log_path}\")\n",
                "print(df_log[['File_Name', 'Status', 'Reset_Detected', 'Sync_Strategy']].to_string())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}